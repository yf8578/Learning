{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一周"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "爬虫库：\n",
    "1. 小规模，数据量小，速度不敏感：Requests\n",
    "2. 中规模，较大，速度敏感：Scrapy\n",
    "3. 大规模，搜索引擎：定制开发\n",
    "爬虫限制：\n",
    "1. 来源审查：判断User-Agent字段\n",
    "2. 发布公告：Robots协议，告知所有爬虫网站的爬取策略，要求遵守，如:https://www.douban.com/robots.txt\n",
    "\n",
    "Robots协议使用：\n",
    "类人行为可不参考"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Requests库安装\n",
    "    - 主要有7个主要方法\n",
    "        - requests.request() 构造一个请求，支撑以下各方法的基础方法，其他六个调用该方法实现\n",
    "        - requests.get() 获取HTML网页的主要方法，对应于HTTP的GET.......最常用\n",
    "        - requests.head() 获取HTML网页头信息的方法，对应于HTTP的HEAD\n",
    "        - requests.post() 向HTML网页提交POST请求的方法，对应于HTTP的POST\n",
    "        - requests.put() 向HTML网页提交PUT请求的方法，对应于HTTP的PUT\n",
    "        - requests.patch() 向HTML网页提交局部修改请求，对应于HTTP的PATCH\n",
    "        - requests.delete() 向HTML页面提交删除请求，对应于HTTP的DELETE\n",
    "    - 六种异常\n",
    "      - requests.ConnectionError 网络连接错误异常，如DNS查询失败、拒绝连接等\n",
    "      - requests.HTTPError HTTP错误异常\n",
    "      - requests.URLRequired URL缺失异常\n",
    "      - requests.TooManyRedirects 超过最大重定向次数，产生重定向异常\n",
    "      - requests.ConnectTimeout 连接远程服务器超时异常（远程服务器链接异常）\n",
    "      - requests.Timeout 请求URL超时，产生超时异常(发出请求到获得内容的整个过程异常)\n",
    "      - r.raise_for_status() 如果不是200，产生异常requests.HTTPError,如果是200，什么都不做,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "<head>\n",
      "    <meta charset=\"UTF-8\">\n",
      "    <title>Zhangyf</title>\n",
      "</head>\n",
      "<body>\n",
      "    <h1>Zhangyf--test的个人主页</h1>\n",
      "    <h1>Hello ~·······\n",
      "        这是一件悲伤的事\n",
      "    </h1>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#代码框架\n",
    "import requests\n",
    "\n",
    "def getHTMLText(url):\n",
    "    try:\n",
    "        r = requests.get(url, timeout=30)\n",
    "        r.raise_for_status()#如果状态不是200，引发HTTPError异常\n",
    "        r.encoding = r.apparent_encoding\n",
    "        return r.text\n",
    "    except:\n",
    "        return \"产生异常\"\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    url = \"http://yf8578.github.io\"\n",
    "    print(getHTMLText(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'Connection': 'keep-alive', 'Content-Length': '240', 'Server': 'GitHub.com', 'Content-Type': 'text/html; charset=utf-8', 'permissions-policy': 'interest-cohort=()', 'Last-Modified': 'Thu, 27 Jul 2023 03:13:43 GMT', 'Access-Control-Allow-Origin': '*', 'Strict-Transport-Security': 'max-age=31556952', 'ETag': '\"64c1e0e7-f0\"', 'expires': 'Fri, 28 Jul 2023 06:23:42 GMT', 'Cache-Control': 'max-age=600', 'x-proxy-cache': 'MISS', 'X-GitHub-Request-Id': 'A962:06E5:3ECA9E:41FBED:64C35C95', 'Accept-Ranges': 'bytes', 'Date': 'Fri, 28 Jul 2023 06:33:57 GMT', 'Via': '1.1 varnish', 'Age': '334', 'X-Served-By': 'cache-nrt-rjtf7700023-NRT', 'X-Cache': 'HIT', 'X-Cache-Hits': '1', 'X-Timer': 'S1690526038.637932,VS0,VE1', 'Vary': 'Accept-Encoding', 'X-Fastly-Request-ID': '52282d49ca83dc2b7963b7f18447c47bfc307344'}\n"
     ]
    }
   ],
   "source": [
    "r=requests.get('https://yf8578.github.io')\n",
    "print(r.status_code)\n",
    "r.encoding='utf-8'\n",
    "print(r.headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### resquests.get(url)\n",
    "\n",
    "    通过resuests.get(url)构造一个向服务器请求资源的Request对象，返回一个包含服务器资源的Response对象。\n",
    "    \n",
    "```\n",
    "requests.get(url, params=None, **kwargs)\n",
    "# url: 拟获取页面的url链接\n",
    "# params: url中的额外参数，字典或字节流格式，可选\n",
    "# **kwargs: 12个控制访问的参数\n",
    "```\n",
    "\n",
    "Response对象的属性：\n",
    "1. r.status_code: HTTP请求的返回状态，200表示连接成功，404表示失败\n",
    "2. r.text: HTTP响应内容的字符串形式，即url对应的页面内容\n",
    "3. r.encoding: 从HTTP header中猜测的响应内容编码方式,charset中获得，如果header中不存在charset，则认为编码为ISO-8859-1\n",
    "4. r.apparent_encoding: 从内容中分析出的响应内容编码方式（备选编码方式）\n",
    "5. r.content: HTTP响应内容的二进制形式(图片、视频等)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTTP协议\n",
    "HTTP，Hypertext Transfer Protocol，超文本传输协议，是一个基于TCP/IP通信协议来传递数据（HTML 文件, 图片文件, 查询结果等）。基于“请求与响应”模式的、无状态的应用层协议。\n",
    "HTTP协议采用URL作为定位网络资源的标识，URL格式如下：\n",
    "```\n",
    "http://host[\":\"port][abs_path]\n",
    "```\n",
    "其中：\n",
    "- host：合法的Internet主机域名或者IP地址\n",
    "- port：端口号，缺省端口为80\n",
    "- path：请求资源的路径\n",
    "\n",
    "对资源的操作(和requstes中操作相对应)：\n",
    "  - GET：请求获取URL位置的资源\n",
    "  - HEAD：请求获取URL位置资源的响应消息报告，即获得该资源的头部信息\n",
    "  - POST：请求向URL位置的资源后附加新的数据\n",
    "  - PUT：请求向URL位置存储一个资源，覆盖原URL位置的资源\n",
    "  - PATCH：请求局部更新URL位置的资源，即改变该处资源的部分内容\n",
    "  - DELETE：请求删除URL位置存储的资源\n",
    "#### PATCH和PUT的区别\n",
    "假设URL位置又一组数据，UserInfo，包括UserID、UserName等20个字段。\n",
    "需求：用户修改了UserName，其他不变。\n",
    "采用PATCH，仅向URL提交UserName的局部更新请求，服务器根据请求，在数据库中找到该UserName，将其更新为请求所附数据，其他字段不变。\n",
    "采用PUT，必须将所有20个字段一并提交给服务器，服务器将URL位置的资源全部替换为请求所附数据，如果某个字段未提交数据，则该字段被置空。节省网络带宽"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requests库的主要解析方法\n",
    "#### requests.request(method, url, **kwargs)\n",
    "method:请求方式，对应get/put/post等7种\n",
    "url:请求的url\n",
    "**kwargs:控制访问的参数，共13个\n",
    "##### method:请求方式\n",
    "```\n",
    "r=requests.request('GET',url,**kwargs)\n",
    "r=requests.request('POST',url,**kwargs)\n",
    "r=requests.request('PUT','url,**kwargs)\n",
    "r=requests.request('HEAD',url,**kwargs)\n",
    "r=requests.request('DELETE',url,**kwargs)\n",
    "r=requests.request('OPTIONS',url,**kwargs)\n",
    "r=requests.request('PATCH',url,**kwargs)  #获取服务器相关参数\n",
    "```\n",
    "##### **kwargs:控制访问参数\n",
    "params:字典或字节序列，作为参数增加到url中\n",
    "data:字典、字节序列或文件对象，作为Request的内容\n",
    "json:JSON格式的数据，作为Request的内容\n",
    "headers:字典，HTTP定制头\n",
    "cookies:字典或CookieJar，Request中的cookie\n",
    "auth:元组，支持HTTP认证功能\n",
    "files:字典类型，传输文件\n",
    "timeout:设定超时时间，秒为单位\n",
    "proxies:字典类型，设定访问代理服务器，可以增加登录认证\n",
    "allow_redirects:True/False，默认为True，重定向开关\n",
    "stream:True/False，默认为True，获取内容立即下载开关\n",
    "verify:True/False，默认为True，认证SSL证书开关\n",
    "cert:本地SSL证书路径\n",
    "\n",
    "#### requests.get(url,,parames=None,**kwargs)\n",
    "url:拟获取页面的url链接\n",
    "params:url中的额外参数，字典或字节流格式，可选\n",
    "**kwargs:12个控制访问参数\n",
    "\n",
    "#### requests.head(url,**kwargs)\n",
    "url:拟获取页面的url链接\n",
    "**kwargs:13个控制访问参数,request一样\n",
    "\n",
    "#### requests.post(url,data=None,json=None,**kwargs)\n",
    "url:拟更新页面的url链接\n",
    "data:字典、字节序列或文件对象，作为Request的内容\n",
    "json:JSON格式的数据，作为Request的内容\n",
    "**kwargs:11个控制访问参数,request一样\n",
    "\n",
    "#### requests.put(url,data=None,**kwargs)\n",
    "url:拟更新页面的url链接\n",
    "data:字典、字节序列或文件对象，作为Request的内容\n",
    "**kwargs:12个控制访问参数,request一样\n",
    "\n",
    "......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requests库网络爬虫实战"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "产生异常\n"
     ]
    }
   ],
   "source": [
    "# 京东商品爬取\n",
    "import requests\n",
    "\n",
    "def getHTMLText(url):\n",
    "    try:\n",
    "        r = requests.get(url, timeout=30)\n",
    "        r.raise_for_status()#如果状态不是200，引发HTTPError异常\n",
    "        r.encoding = r.apparent_encoding\n",
    "        return r.text\n",
    "    except:\n",
    "        return \"产生异常\"\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    url = \"https://item.jd.com/100000177760.html\"\n",
    "    print(getHTMLText(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://wappass.baidu.com/static/captcha/tuxing.html?&logid=11315708929519608216&ak=c27bbc89afca0463650ac9bde68ebe06&backurl=https%3A%2F%2Fwww.baidu.com%2Fs%3Fwd%3Dkeyword&ext=x9G9QDmMXq%2FNo87gjGO0PyhLVOUqsaAxAwFZXibsbhQ9qxHzv8tRxodBg5SciE9xg1T9i%2F9aWxTqDk%2BHyX%2BPs%2ForrljiTp84O4n5EMuWAbAiuviuLrZiNtUDP6frRHN4SCAe1EQ3IsG%2BFn2u6CQ8X5MGpZvluuS3axXkT94wgh4%3D&signature=e23c3910de5348d2c8ec8210f93eea93&timestamp=1690530616\n",
      "1488\n"
     ]
    }
   ],
   "source": [
    "# 百度、360搜索关键字提交\n",
    "# 百度关键字接口：http://www.baidu.com/s?wd=keyword\n",
    "# 360关键字接口：http://www.so.com/s?q=keyword\n",
    "\n",
    "import requests\n",
    "keyword = \"Python\"\n",
    "try:\n",
    "    kv={'wd':'keyword'}\n",
    "    r=requests.get(\"http://www.baidu.com/s\",params=kv)\n",
    "    print(r.request.url)\n",
    "    r.raise_for_status()\n",
    "    print(len(r.text))\n",
    "except:\n",
    "    print(\"爬取失败\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "爬取失败\n"
     ]
    }
   ],
   "source": [
    "#网络图片爬取存储,图片成功\n",
    "#视频失败\n",
    "# http://clips.vorwaerts-gmbh.de/big_buck_bunny.mp4\n",
    "import requests\n",
    "import os\n",
    "path='D:\\\\000zyf\\\\Learning\\\\python_learn\\\\spider\\\\test.mp4'\n",
    "url=r\"http://clips.vorwaerts-gmbh.de/big_buck_bunny.mp4\"\n",
    "#模拟谷歌浏览器\n",
    "hd={'user-agent':'Mozilla/5.0'}\n",
    "try:\n",
    "    if not os.path.exists(path):\n",
    "        r=requests.get(url,headers=hd)\n",
    "        print(r.status_code)\n",
    "        with open(path,'wb') as f:\n",
    "            f.write(r.content)\n",
    "            f.close()\n",
    "            print(\"文件保存成功\")\n",
    "    else:\n",
    "        print(\"文件已存在\")\n",
    "except:\n",
    "    print(\"爬取失败\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IP地址归属地查询\n",
    "import requests\n",
    "#不管用啦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pachong",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
