{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scrapy 框架"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "功能强大的网络爬虫框架，第三方库，需要安装\n",
    "<br />\n",
    "安装后可使用scrapy -h命令测试是否安装成功\n",
    "<br />\n",
    "爬虫框架是实现爬虫功能的一个软件结构和功能组件集合，是一个半成品，能够帮助用户实现专业的网络爬虫。该框架约束了使用模板。\n",
    "<br />\n",
    "7个组件（5+2结构，5个部分是框架的主体部分，2个“中间键”）：\n",
    "<br />\n",
    "1.引擎（engine）：用来处理整个系统的数据流处理，触发事务，是整个系统的“调度中心”\n",
    "<br />\n",
    "2.调度器（scheduler）：用来接受引擎发过来的请求，压入队列中，并在引擎再次请求的时候返回，是引擎的“请求入口”\n",
    "<br />\n",
    "3.下载器（downloader）：用于下载网页内容，并将网页内容返回给蜘蛛，是引擎的“下载器”\n",
    "<br />\n",
    "4.爬虫（spider）：爬虫是主要干活的，用于从特定的网页中提取自己需要的信息，即所谓的实体（item），用户也可以从中提取出链接，让Scrapy继续抓取下一个页面，是引擎的“实体提取器”\n",
    "<br />\n",
    "5.项目管道（pipeline）：负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。典型的处理有清理HTML数据、验证爬取数据（检查item是否包含某些字段）、查重（并丢弃）以及将爬取结果保存到数据库中。它的主要功能是清洗、验证和存储数据。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。典型的处理有清理HTML数据、验证爬取数据（检查item是否包含某些字段）、查重（并丢弃）以及将爬取结果保存到数据库中。\n",
    "<br />\n",
    "PS：在引擎和爬虫之间有两个主要的中间件（Downloader Middlewares）和（Spider Middlewares）。\n",
    "<br />\n",
    "6.下载器中间件（Downloader Middlewares）：实施engine，scheduler和downloader之间用户可配置的控制，通过编写可以修改、丢弃、新增请求或相应\n",
    "<br />\n",
    "7.爬虫中间件（Spider Middlewares）：实施engine，scheduler和spider之间用户可配置的控制，通过编写可以修改、丢弃、新增请求或相应\n",
    "<br />\n",
    "**重点编写的是sipders和items pipeline模块**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy框架包含三个主要的数据流路径：\n",
    "<br />\n",
    "1. spider→engine（从spiders获取用户的request，可简单理解为url）→scheduler（对爬取请求进行调度）\n",
    "<br />\n",
    "2. scheduler→engine（将爬取请求入队）→downloader（链接互联网，下载网页内容，形成对象response）→engine（将response返回给spider）→spider\n",
    "<br />\n",
    "spiders（response处理成items和requests）→engine（将爬取到的item给engine）→item pipeline（处理item）以及scheduler模块（处理requests）\n",
    "<br />\n",
    "**在这个框架中入口时spiders，出口时item pipelines**\n",
    "<br />\n",
    "engine，scheduler，downloader已有实现，spiders和item pipelines需要用户实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比较request和scrapy：\n",
    "<br />\n",
    "相同点：\n",
    "<br />\n",
    "- 两者都可以进行页面请求和爬取，python爬虫的两个重要技术路线\n",
    "- 两者可用性都很好，文档丰富，入门简单\n",
    "- 两者都没有处理js、提交表单、因对验证码等功能（可拓展）\n",
    "不同点：\n",
    "<br />\n",
    "- request是页面级爬虫（爬取小量网站），scrapy是网站级爬虫（爬取大量网站）\n",
    "- request是python第三方库，scrapy是python第三方框架\n",
    "- 并发性考虑不足，性能较差；并发性好，性能较高\n",
    "- 重点在于页面下载；重点在于爬虫结构\n",
    "- 定制灵活；一般定制灵活，深度定制困难\n",
    "- 上手十分简单；入门稍难"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spider",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
